{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING  torch.Tensor inputs should be normalized 0.0-1.0 but max value is 255.0. Dividing input by 255.\n",
      "0: 512x512 14 cars, 2 longvehicles, 22.0ms\n",
      "Speed: 0.0ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 512)\n",
      "\n",
      "WARNING  torch.Tensor inputs should be normalized 0.0-1.0 but max value is 255.0. Dividing input by 255.\n",
      "0: 512x512 6 persons, 23 cars, 3 longvehicles, 23.0ms\n",
      "Speed: 0.0ms preprocess, 23.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 512)\n",
      "\n",
      "WARNING  torch.Tensor inputs should be normalized 0.0-1.0 but max value is 255.0. Dividing input by 255.\n",
      "0: 96x512 3 cars, 21.5ms\n",
      "Speed: 0.0ms preprocess, 21.5ms inference, 1.0ms postprocess per image at shape (1, 3, 96, 512)\n",
      "\n",
      "WARNING  torch.Tensor inputs should be normalized 0.0-1.0 but max value is 244.0. Dividing input by 255.\n",
      "0: 96x512 (no detections), 25.0ms\n",
      "Speed: 0.0ms preprocess, 25.0ms inference, 0.0ms postprocess per image at shape (1, 3, 96, 512)\n",
      "Final Detections: [[     398.22      440.03      498.42      493.44     0.93618           1]\n",
      " [     363.93      307.44      445.18      343.35     0.92784           1]\n",
      " [     496.71      289.54      535.51      332.68     0.92779           1]\n",
      " [     211.46         480      324.98      539.14     0.92773           1]\n",
      " [     193.24      399.57      289.22      451.55     0.91905           1]\n",
      " [      223.5      430.58      326.46      474.85     0.91734           1]\n",
      " [      824.8      94.229      902.05      126.52      0.9158           2]\n",
      " [        216      462.36      331.24      511.86     0.91557           1]\n",
      " [     789.39      202.94      827.11      227.41     0.91532           1]\n",
      " [     480.05      253.63       515.1      285.83     0.89095           1]\n",
      " [     523.93      241.12      551.45      276.73     0.88826           1]\n",
      " [     531.75       290.5      572.02      328.13     0.88378           1]\n",
      " [     711.54      140.35      736.85      158.33     0.87936           1]\n",
      " [     689.96      99.743      718.82      114.06     0.87884           1]\n",
      " [     455.41       296.2       487.8      339.15     0.87803           1]\n",
      " [      464.4      183.95         512      204.58      0.8747           1]\n",
      " [     324.57      285.42      359.72      325.57     0.87057           1]\n",
      " [     484.62      198.66      511.96      226.28     0.87048           1]\n",
      " [     703.89      73.868      732.79      88.027     0.85052           1]\n",
      " [     484.16      82.988      511.94      99.778     0.83709           1]\n",
      " [     686.94      78.812      719.56       91.54     0.83648           1]\n",
      " [     491.42      298.55         512      335.37     0.82504           1]\n",
      " [     291.71      292.53      320.94      327.18     0.81676           1]\n",
      " [     861.03      408.55      880.14      436.44     0.79773           0]\n",
      " [     505.99      38.435      518.58      50.944     0.78735           1]\n",
      " [     792.01       92.28      846.61      119.45     0.77623           2]\n",
      " [     486.03      22.158      496.54      31.844     0.76765           1]\n",
      " [     550.13      59.105      567.16      73.054     0.76533           1]\n",
      " [      497.2      48.712      508.71      59.286     0.74233           1]\n",
      " [     506.25      61.425      521.25      73.122     0.72414           1]\n",
      " [     613.29      62.849      631.76      75.185     0.65607           1]\n",
      " [     480.04      15.761      486.16      23.876     0.58612           1]\n",
      " [     481.98      40.405      495.22      50.732     0.57803           1]\n",
      " [     501.64      26.417      511.45      35.171     0.57679           1]\n",
      " [     158.11      530.18      289.83      572.14     0.57529           1]\n",
      " [     525.57      56.814      541.91      72.987     0.56854           1]\n",
      " [     499.65       55.36      511.94      66.037     0.56686           1]\n",
      " [     632.07      94.653      637.75      105.79     0.52684           0]\n",
      " [     118.19      187.05      221.75       225.3     0.47371           2]\n",
      " [     543.56      139.63      549.47      152.18     0.47035           0]\n",
      " [     612.04      54.335      625.61      63.897     0.41661           1]\n",
      " [     480.04      298.32      493.24      336.09     0.40584           2]\n",
      " [     516.65      99.058      521.96       110.9     0.40505           0]\n",
      " [     534.39      101.87      539.48      115.77     0.38385           0]\n",
      " [     519.99      51.266      532.96      64.719     0.38208           1]\n",
      " [     514.58       48.97      529.94      60.202      0.2936           1]\n",
      " [     549.74      138.82      556.79      151.92     0.26558           0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO # Assuming this is your YOLOv10 implementation\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # To ensure GPU usage\n",
    "\n",
    "# Step 1: Slice Image\n",
    "def slice_image(image, slice_size, overlap):\n",
    "    \"\"\"\n",
    "    Divide the image into overlapping slices.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image.\n",
    "        slice_size (int): Size of each slice (width and height).\n",
    "        overlap (float): Overlap ratio between slices (0 to 1).\n",
    "\n",
    "    Returns:\n",
    "        slices (list): List of image slices.\n",
    "        coordinates (list): List of top-left coordinates of each slice.\n",
    "    \"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    step = int(slice_size * (1 - overlap))\n",
    "    slices = []\n",
    "    coordinates = []\n",
    "\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            slice_img = image[y:y + slice_size, x:x + slice_size]\n",
    "            slices.append(slice_img)\n",
    "            coordinates.append((x, y))\n",
    "\n",
    "    return slices, coordinates\n",
    "\n",
    "# Step 2: Infer Slices\n",
    "def resize_to_nearest_compatible_size(image, target_size=480):\n",
    "    \"\"\"\n",
    "    Resize the image to the nearest size divisible by 32.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image to be resized.\n",
    "        target_size (int): Target size for resizing (e.g., 640).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Resized image with dimensions divisible by 32.\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_width = (width // 32) * 32\n",
    "    new_height = (height // 32) * 32\n",
    "    \n",
    "    if new_width < target_size:\n",
    "        new_width += 32\n",
    "    if new_height < target_size:\n",
    "        new_height += 32\n",
    "    \n",
    "    return cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "def infer_slices(model, slices):\n",
    "    detections = []\n",
    "    for slice_img in slices:\n",
    "        # Resize the slice to the nearest compatible size (divisible by 32)\n",
    "        resized_slice = resize_to_nearest_compatible_size(slice_img, target_size=640)\n",
    "        \n",
    "        # Convert image to tensor and move to GPU\n",
    "        tensor_img = torch.from_numpy(resized_slice).permute(2, 0, 1).unsqueeze(0).float().cuda()\n",
    "        \n",
    "        # Run inference\n",
    "        results = model.predict(tensor_img)  # Perform inference\n",
    "        \n",
    "        # Extract bounding boxes from the results object\n",
    "        boxes = results[0].boxes  # This is where the boxes are stored\n",
    "        \n",
    "        # Get the boxes in [x1, y1, x2, y2, confidence, class_id] format\n",
    "        slice_detections = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()  # Convert to numpy array\n",
    "            confidence = box.conf[0].cpu().numpy()  # Confidence score\n",
    "            class_id = int(box.cls[0].cpu().numpy())  # Class ID\n",
    "            \n",
    "            slice_detections.append([x1, y1, x2, y2, confidence, class_id])\n",
    "        \n",
    "        detections.append(slice_detections)\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Aggregate Results\n",
    "def aggregate_results(detections, coordinates, slice_size, image_shape, iou_threshold):\n",
    "    all_detections = []\n",
    "\n",
    "    for det, coord in zip(detections, coordinates):\n",
    "        x_offset, y_offset = coord\n",
    "        for bbox in det:\n",
    "            x1, y1, x2, y2, score, class_id = bbox\n",
    "            # Map detections back to the original image coordinates\n",
    "            x1 += x_offset\n",
    "            y1 += y_offset\n",
    "            x2 += x_offset\n",
    "            y2 += y_offset\n",
    "            all_detections.append([x1, y1, x2, y2, score, class_id])\n",
    "\n",
    "    # Convert to numpy array\n",
    "    all_detections = np.array(all_detections)\n",
    "\n",
    "    # Apply Non-Max Suppression\n",
    "    final_detections = non_max_suppression(all_detections, iou_threshold)\n",
    "\n",
    "    return final_detections\n",
    "\n",
    "\n",
    "# Utility: Non-Max Suppression\n",
    "def non_max_suppression(detections, iou_threshold):\n",
    "    \"\"\"\n",
    "    Apply Non-Max Suppression (NMS) to filter overlapping boxes.\n",
    "\n",
    "    Args:\n",
    "        detections (numpy.ndarray): Array of detections.\n",
    "        iou_threshold (float): IOU threshold for NMS.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Filtered detections after NMS.\n",
    "    \"\"\"\n",
    "    if len(detections) == 0:\n",
    "        return detections\n",
    "\n",
    "    x1 = detections[:, 0]\n",
    "    y1 = detections[:, 1]\n",
    "    x2 = detections[:, 2]\n",
    "    y2 = detections[:, 3]\n",
    "    scores = detections[:, 4]\n",
    "\n",
    "    indices = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        current = indices[0]\n",
    "        keep.append(current)\n",
    "        rest = indices[1:]\n",
    "\n",
    "        # Compute IOU\n",
    "        xx1 = np.maximum(x1[current], x1[rest])\n",
    "        yy1 = np.maximum(y1[current], y1[rest])\n",
    "        xx2 = np.minimum(x2[current], x2[rest])\n",
    "        yy2 = np.minimum(y2[current], y2[rest])\n",
    "\n",
    "        inter_area = np.maximum(0, xx2 - xx1) * np.maximum(0, yy2 - yy1)\n",
    "        box_area = (x2 - x1) * (y2 - y1)\n",
    "        union_area = box_area[current] + box_area[rest] - inter_area\n",
    "\n",
    "        iou = inter_area / (union_area + 1e-6)\n",
    "        indices = rest[iou < iou_threshold]\n",
    "\n",
    "    return detections[keep]\n",
    "\n",
    "def annotate_image(image, detections):\n",
    "    \"\"\"\n",
    "    Annotates the image with bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): The original image.\n",
    "        detections (list): List of detections with format [x1, y1, x2, y2, confidence, class_id].\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The annotated image with bounding boxes.\n",
    "    \"\"\"\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, score, class_id = detection\n",
    "        \n",
    "        # Draw rectangle\n",
    "        color = (0, 255, 0)  # Green color for bounding box (you can choose different colors)\n",
    "        thickness = 2\n",
    "        image = cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)\n",
    "        \n",
    "        # Draw label (class name + confidence)\n",
    "        # make the label font blue and thinner\n",
    "        label = f\"{model.names[class_id]} {score:.2f}\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.4\n",
    "        font_thickness = 1\n",
    "        label_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]\n",
    "        label_x = int(x1)\n",
    "        label_y = int(y1) - 10\n",
    "        image = cv2.putText(image, label, (label_x, label_y), font, font_scale, color, font_thickness)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    img_path = r\"C:\\Users\\Abhim\\OneDrive\\Pictures\\Screenshots\\test_img.png\"\n",
    "    image = cv2.imread(img_path)\n",
    "    slice_size = 480\n",
    "    overlap = 0\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "    # Initialize YOLOv10 model\n",
    "    path = r\"C:\\Users\\Abhim\\Downloads\\yolov10l_81eps_960.pt\"\n",
    "    # path = r\"C:\\Users\\Abhim\\Downloads\\yolov10l_81eps_960_quantized_int8.pt\"\n",
    "    \n",
    "    model = YOLO(path).cuda()  # Ensure the model is on GPU\n",
    "\n",
    "    #resize the image so that the width is 960 and the height is scaled accordingly\n",
    "    image = cv2.resize(image, (960, int(960 * image.shape[0] / image.shape[1])))\n",
    "\n",
    "    # Slice the image\n",
    "    slices, coordinates = slice_image(image, slice_size, overlap)\n",
    "\n",
    "    # Perform inference on slices\n",
    "    detections = infer_slices(model, slices)\n",
    "\n",
    "    # Aggregate results\n",
    "    final_detections = aggregate_results(detections, coordinates, slice_size, image.shape, iou_threshold)\n",
    "    annotated_image = annotate_image(image.copy(), final_detections)\n",
    "\n",
    "    # Display the annotated image\n",
    "    cv2.imshow(\"Annotated Image\", annotated_image)\n",
    "    cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Optionally, save the annotated image to a file\n",
    "    cv2.imwrite(\"annotated_image.png\", annotated_image)\n",
    "\n",
    "    print(\"Final Detections:\", final_detections)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
